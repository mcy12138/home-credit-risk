{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#change to your working directory\n",
    "os.chdir('C:/Users/aliks/Desktop/Teaching/U of T Teaching/MMF Course/2021 Course')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basics of torch:\n",
    "\n",
    "#We make two random tensors to inspect; these could represent, for example, time series data\n",
    "\n",
    "#Dimension: batch size (3) x sequence length (28) * number of time series / features (4)\n",
    "# e.g. 4 28-tick financial data time series in batches of 3\n",
    "\n",
    "x1 = torch.rand(3, 28, 4)\n",
    "x2 = torch.rand(3, 28, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[9.3087e-01, 1.7315e-01, 2.8236e-01, 9.3444e-01],\n",
       "         [2.5429e-01, 8.4608e-01, 4.4174e-01, 1.4539e-01],\n",
       "         [3.0301e-01, 7.4598e-01, 9.3455e-01, 9.8392e-01],\n",
       "         [2.9832e-04, 3.0653e-01, 4.5819e-01, 8.7662e-01],\n",
       "         [6.1554e-01, 3.9989e-01, 8.4597e-01, 2.1477e-01],\n",
       "         [4.6032e-01, 3.2748e-01, 4.7584e-02, 6.9092e-02],\n",
       "         [1.4538e-01, 2.1016e-01, 3.9173e-01, 6.2923e-01],\n",
       "         [3.6042e-01, 2.0448e-02, 8.0906e-01, 6.0462e-01],\n",
       "         [2.0630e-02, 8.9155e-01, 5.1732e-01, 9.7579e-01],\n",
       "         [2.1867e-01, 5.3230e-01, 9.2079e-01, 3.5374e-01],\n",
       "         [2.0026e-01, 1.0386e-01, 5.3558e-01, 8.1438e-01],\n",
       "         [9.9730e-01, 6.9433e-01, 1.1051e-01, 3.9944e-01],\n",
       "         [8.8416e-01, 1.0234e-01, 5.1960e-02, 3.0556e-01],\n",
       "         [8.5358e-01, 8.8885e-01, 9.8743e-01, 4.1355e-01],\n",
       "         [1.4834e-01, 2.2713e-01, 8.0154e-01, 3.5502e-01],\n",
       "         [6.9338e-01, 8.3720e-01, 7.4918e-01, 3.0379e-01],\n",
       "         [4.8672e-01, 8.3447e-01, 8.7445e-01, 4.9836e-02],\n",
       "         [2.1355e-01, 4.5486e-02, 6.3044e-01, 8.4847e-01],\n",
       "         [2.9324e-01, 4.3056e-01, 8.3198e-01, 8.4292e-01],\n",
       "         [9.9442e-01, 7.8866e-01, 2.5306e-01, 6.0858e-01],\n",
       "         [3.2305e-01, 4.8316e-01, 2.2775e-01, 4.9390e-01],\n",
       "         [9.7927e-01, 9.6213e-01, 8.8426e-01, 4.2382e-01],\n",
       "         [9.4824e-01, 8.8014e-02, 9.2886e-01, 1.3339e-03],\n",
       "         [1.1371e-02, 4.0382e-01, 4.4764e-01, 3.6095e-01],\n",
       "         [3.3048e-01, 2.9616e-02, 7.6859e-01, 9.3123e-01],\n",
       "         [7.7656e-01, 3.8571e-01, 8.3268e-01, 8.7091e-01],\n",
       "         [3.8378e-01, 2.8605e-01, 8.2222e-01, 3.7116e-01],\n",
       "         [2.0335e-01, 7.9428e-01, 4.9958e-01, 1.4404e-01]],\n",
       "\n",
       "        [[6.6377e-01, 2.9646e-01, 8.9375e-01, 2.3359e-01],\n",
       "         [9.4910e-01, 4.0550e-01, 9.8157e-01, 7.9272e-01],\n",
       "         [8.6342e-01, 9.0632e-01, 9.9018e-01, 8.6850e-01],\n",
       "         [9.1628e-01, 5.0604e-01, 5.1419e-01, 8.6151e-01],\n",
       "         [2.6028e-01, 4.5193e-01, 6.2475e-01, 4.8142e-02],\n",
       "         [9.7403e-01, 2.4822e-01, 3.7380e-01, 7.6817e-01],\n",
       "         [4.2505e-01, 1.7205e-01, 9.9417e-01, 6.0359e-01],\n",
       "         [2.5367e-01, 4.7762e-01, 6.6577e-01, 5.7947e-02],\n",
       "         [6.5225e-01, 4.3742e-01, 3.3360e-01, 1.7515e-01],\n",
       "         [2.5944e-01, 9.6069e-02, 1.5754e-01, 7.6049e-01],\n",
       "         [2.8287e-01, 4.4385e-01, 6.9467e-02, 9.2625e-01],\n",
       "         [5.9846e-01, 9.6441e-01, 7.8560e-01, 6.9709e-02],\n",
       "         [7.6382e-02, 2.0981e-01, 6.4013e-01, 7.8872e-01],\n",
       "         [2.1815e-03, 1.8787e-01, 4.8800e-01, 2.8478e-01],\n",
       "         [1.6542e-01, 3.5217e-02, 3.9189e-01, 5.7041e-01],\n",
       "         [4.7889e-01, 9.2300e-01, 6.8762e-01, 7.8613e-01],\n",
       "         [4.1868e-01, 7.0286e-01, 3.1212e-01, 3.0355e-01],\n",
       "         [4.9791e-01, 6.5851e-01, 8.3283e-01, 1.0466e-02],\n",
       "         [7.2213e-01, 3.4102e-01, 5.2324e-01, 3.5595e-01],\n",
       "         [9.0639e-01, 3.6979e-01, 1.5786e-01, 3.9149e-01],\n",
       "         [1.4875e-01, 6.1562e-01, 7.7304e-01, 8.9933e-01],\n",
       "         [9.3144e-01, 4.7276e-01, 9.1402e-01, 7.9818e-01],\n",
       "         [2.3145e-01, 7.0967e-01, 5.4861e-01, 9.2515e-01],\n",
       "         [5.2425e-01, 5.1061e-01, 4.0358e-01, 2.9349e-01],\n",
       "         [1.8520e-01, 7.7876e-01, 2.7130e-02, 8.2978e-01],\n",
       "         [2.7581e-02, 2.6286e-01, 9.4540e-01, 3.6732e-01],\n",
       "         [2.8121e-01, 6.9391e-01, 9.8609e-01, 4.4175e-01],\n",
       "         [9.7117e-01, 1.7077e-01, 1.5024e-01, 6.8203e-01]],\n",
       "\n",
       "        [[7.5602e-01, 5.5114e-01, 9.0751e-01, 8.9564e-01],\n",
       "         [7.6687e-01, 5.4659e-01, 4.2328e-01, 2.1906e-01],\n",
       "         [8.3163e-01, 4.5823e-01, 2.6755e-01, 4.4258e-01],\n",
       "         [7.1390e-01, 5.8954e-01, 7.4990e-01, 4.8725e-01],\n",
       "         [2.1079e-01, 1.3351e-01, 9.2234e-01, 8.4785e-01],\n",
       "         [6.7985e-01, 7.4077e-01, 2.1354e-01, 9.4401e-01],\n",
       "         [6.7855e-01, 8.2149e-03, 3.6430e-01, 7.4840e-01],\n",
       "         [2.6963e-01, 3.4324e-01, 9.0856e-01, 7.8233e-01],\n",
       "         [7.5847e-01, 3.4667e-01, 6.4918e-01, 9.3165e-01],\n",
       "         [9.3410e-01, 6.8066e-01, 8.9956e-01, 9.0198e-01],\n",
       "         [4.1957e-01, 1.5907e-01, 9.7529e-03, 6.2314e-01],\n",
       "         [7.9810e-01, 3.8603e-01, 7.6191e-01, 2.2830e-01],\n",
       "         [3.8187e-01, 2.2527e-01, 1.3111e-01, 5.9162e-01],\n",
       "         [7.5423e-01, 5.2426e-02, 6.3125e-01, 8.9468e-01],\n",
       "         [8.5873e-01, 7.9801e-01, 2.9578e-01, 7.4008e-01],\n",
       "         [2.7380e-01, 6.9595e-01, 2.0833e-01, 5.9596e-01],\n",
       "         [7.8800e-03, 4.1994e-01, 2.6788e-01, 9.9508e-01],\n",
       "         [6.0985e-01, 2.3896e-01, 8.3037e-01, 2.3552e-01],\n",
       "         [9.1351e-01, 4.6136e-02, 7.7534e-01, 1.9718e-01],\n",
       "         [6.3787e-01, 9.5992e-01, 8.9533e-01, 2.0081e-01],\n",
       "         [5.9077e-01, 4.1692e-01, 2.9506e-01, 5.2678e-01],\n",
       "         [6.2398e-01, 7.7558e-01, 5.6972e-01, 5.0016e-01],\n",
       "         [4.9177e-01, 7.2542e-02, 5.3371e-01, 9.4747e-01],\n",
       "         [9.9066e-02, 2.8529e-02, 2.9373e-01, 6.9777e-01],\n",
       "         [7.2305e-01, 5.9720e-01, 5.9912e-01, 1.0721e-01],\n",
       "         [4.0058e-01, 1.3103e-01, 2.8846e-01, 4.5358e-01],\n",
       "         [5.6664e-01, 2.7373e-01, 1.0345e-01, 5.3433e-01],\n",
       "         [2.7692e-01, 6.4454e-01, 9.9026e-01, 2.6401e-01]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 28, 4])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7403, 1.0926, 1.6891, 0.4252],\n",
       "        [1.5379, 0.6892, 1.3155, 1.3267],\n",
       "        [1.6682, 1.2395, 1.8095, 1.5049],\n",
       "        [1.1865, 1.2248, 1.2294, 1.8320],\n",
       "        [1.2033, 1.0076, 0.9300, 1.0007],\n",
       "        [1.9395, 1.0103, 1.0789, 1.4154],\n",
       "        [0.5368, 0.4995, 1.2432, 1.5657],\n",
       "        [0.7552, 1.2549, 0.7524, 0.5821],\n",
       "        [0.7224, 0.5879, 0.7109, 0.4743],\n",
       "        [0.9128, 0.3851, 1.1048, 1.2270],\n",
       "        [0.4099, 0.6872, 0.3367, 1.5844],\n",
       "        [0.6184, 1.7351, 1.4433, 0.2384],\n",
       "        [0.7196, 0.6363, 0.9296, 1.1135],\n",
       "        [0.0943, 0.7520, 0.7543, 0.3588],\n",
       "        [1.0665, 0.6203, 0.4150, 0.6754],\n",
       "        [0.7024, 1.5374, 0.9818, 1.7244],\n",
       "        [1.0780, 0.7062, 0.5887, 1.1344],\n",
       "        [1.2252, 0.8991, 1.5997, 0.3085],\n",
       "        [1.7112, 1.1853, 0.7930, 1.0210],\n",
       "        [1.3894, 1.0239, 0.5325, 1.1807],\n",
       "        [1.0068, 1.5031, 1.3492, 1.2406],\n",
       "        [1.7441, 0.8469, 1.8208, 1.5334],\n",
       "        [0.7761, 1.5084, 1.1387, 1.4636],\n",
       "        [1.2753, 1.4299, 0.9983, 1.0075],\n",
       "        [0.7651, 1.4395, 0.2248, 1.4781],\n",
       "        [0.8340, 0.5013, 1.7187, 0.8895],\n",
       "        [1.0002, 1.5286, 1.3501, 0.7585],\n",
       "        [1.2280, 1.0430, 0.6771, 1.4418]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x1+x2)[1,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 28, 4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x1*x2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.3087e-01, 1.7315e-01, 2.8236e-01, 9.3444e-01],\n",
      "        [2.5429e-01, 8.4608e-01, 4.4174e-01, 1.4539e-01],\n",
      "        [3.0301e-01, 7.4598e-01, 9.3455e-01, 9.8392e-01],\n",
      "        [2.9832e-04, 3.0653e-01, 4.5819e-01, 8.7662e-01],\n",
      "        [6.1554e-01, 3.9989e-01, 8.4597e-01, 2.1477e-01],\n",
      "        [4.6032e-01, 3.2748e-01, 4.7584e-02, 6.9092e-02],\n",
      "        [1.4538e-01, 2.1016e-01, 3.9173e-01, 6.2923e-01],\n",
      "        [3.6042e-01, 2.0448e-02, 8.0906e-01, 6.0462e-01],\n",
      "        [2.0630e-02, 8.9155e-01, 5.1732e-01, 9.7579e-01],\n",
      "        [2.1867e-01, 5.3230e-01, 9.2079e-01, 3.5374e-01],\n",
      "        [2.0026e-01, 1.0386e-01, 5.3558e-01, 8.1438e-01],\n",
      "        [9.9730e-01, 6.9433e-01, 1.1051e-01, 3.9944e-01],\n",
      "        [8.8416e-01, 1.0234e-01, 5.1960e-02, 3.0556e-01],\n",
      "        [8.5358e-01, 8.8885e-01, 9.8743e-01, 4.1355e-01],\n",
      "        [1.4834e-01, 2.2713e-01, 8.0154e-01, 3.5502e-01],\n",
      "        [6.9338e-01, 8.3720e-01, 7.4918e-01, 3.0379e-01],\n",
      "        [4.8672e-01, 8.3447e-01, 8.7445e-01, 4.9836e-02],\n",
      "        [2.1355e-01, 4.5486e-02, 6.3044e-01, 8.4847e-01],\n",
      "        [2.9324e-01, 4.3056e-01, 8.3198e-01, 8.4292e-01],\n",
      "        [9.9442e-01, 7.8866e-01, 2.5306e-01, 6.0858e-01],\n",
      "        [3.2305e-01, 4.8316e-01, 2.2775e-01, 4.9390e-01],\n",
      "        [9.7927e-01, 9.6213e-01, 8.8426e-01, 4.2382e-01],\n",
      "        [9.4824e-01, 8.8014e-02, 9.2886e-01, 1.3339e-03],\n",
      "        [1.1371e-02, 4.0382e-01, 4.4764e-01, 3.6095e-01],\n",
      "        [3.3048e-01, 2.9616e-02, 7.6859e-01, 9.3123e-01],\n",
      "        [7.7656e-01, 3.8571e-01, 8.3268e-01, 8.7091e-01],\n",
      "        [3.8378e-01, 2.8605e-01, 8.2222e-01, 3.7116e-01],\n",
      "        [2.0335e-01, 7.9428e-01, 4.9958e-01, 1.4404e-01]])\n",
      "tensor([[0.3325, 0.9900, 0.2504, 0.4799],\n",
      "        [0.2357, 0.8628, 0.2403, 0.2011],\n",
      "        [0.2981, 0.1715, 0.8410, 0.5047],\n",
      "        [0.6142, 0.4484, 0.5092, 0.8780],\n",
      "        [0.9866, 0.4197, 0.4362, 0.9809],\n",
      "        [0.7831, 0.7931, 0.8965, 0.6346],\n",
      "        [0.4546, 0.4126, 0.4855, 0.6572],\n",
      "        [0.1046, 0.3918, 0.7718, 0.8237],\n",
      "        [0.8431, 0.7012, 0.7790, 0.3465],\n",
      "        [0.5539, 0.5652, 0.3534, 0.8185],\n",
      "        [0.0377, 0.2335, 0.1939, 0.1135],\n",
      "        [0.7884, 0.8501, 0.7252, 0.1461],\n",
      "        [0.7081, 0.6778, 0.5260, 0.1785],\n",
      "        [0.2068, 0.6186, 0.6859, 0.6389],\n",
      "        [0.7520, 0.7380, 0.5529, 0.3356],\n",
      "        [0.4279, 0.6167, 0.5114, 0.9140],\n",
      "        [0.6663, 0.4498, 0.1927, 0.5944],\n",
      "        [0.7505, 0.1018, 0.7472, 0.5745],\n",
      "        [0.4316, 0.8232, 0.3500, 0.0636],\n",
      "        [0.7354, 0.4944, 0.6166, 0.3405],\n",
      "        [0.7573, 0.6936, 0.4216, 0.2564],\n",
      "        [0.8107, 0.9184, 0.6531, 0.6517],\n",
      "        [0.2894, 0.7480, 0.7232, 0.7191],\n",
      "        [0.3614, 0.1288, 0.6291, 0.0635],\n",
      "        [0.5964, 0.6676, 0.9819, 0.1295],\n",
      "        [0.6831, 0.1349, 0.9566, 0.0478],\n",
      "        [0.2258, 0.2244, 0.2747, 0.8827],\n",
      "        [0.9208, 0.0773, 0.8608, 0.9641]])\n"
     ]
    }
   ],
   "source": [
    "print(x1[0])\n",
    "print(x2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.0951e-01, 1.7142e-01, 7.0698e-02, 4.4846e-01],\n",
       "        [5.9942e-02, 7.3000e-01, 1.0613e-01, 2.9244e-02],\n",
       "        [9.0325e-02, 1.2790e-01, 7.8596e-01, 4.9655e-01],\n",
       "        [1.8322e-04, 1.3746e-01, 2.3331e-01, 7.6966e-01],\n",
       "        [6.0727e-01, 1.6784e-01, 3.6902e-01, 2.1066e-01],\n",
       "        [3.6050e-01, 2.5973e-01, 4.2659e-02, 4.3844e-02],\n",
       "        [6.6086e-02, 8.6714e-02, 1.9017e-01, 4.1354e-01],\n",
       "        [3.7695e-02, 8.0110e-03, 6.2445e-01, 4.9805e-01],\n",
       "        [1.7392e-02, 6.2511e-01, 4.0299e-01, 3.3808e-01],\n",
       "        [1.2111e-01, 3.0087e-01, 3.2540e-01, 2.8954e-01],\n",
       "        [7.5485e-03, 2.4254e-02, 1.0385e-01, 9.2448e-02],\n",
       "        [7.8627e-01, 5.9022e-01, 8.0146e-02, 5.8349e-02],\n",
       "        [6.2609e-01, 6.9364e-02, 2.7329e-02, 5.4543e-02],\n",
       "        [1.7655e-01, 5.4987e-01, 6.7727e-01, 2.6422e-01],\n",
       "        [1.1155e-01, 1.6761e-01, 4.4320e-01, 1.1913e-01],\n",
       "        [2.9673e-01, 5.1630e-01, 3.8315e-01, 2.7767e-01],\n",
       "        [3.2430e-01, 3.7537e-01, 1.6850e-01, 2.9621e-02],\n",
       "        [1.6027e-01, 4.6327e-03, 4.7105e-01, 4.8747e-01],\n",
       "        [1.2657e-01, 3.5445e-01, 2.9121e-01, 5.3574e-02],\n",
       "        [7.3128e-01, 3.8995e-01, 1.5603e-01, 2.0722e-01],\n",
       "        [2.4464e-01, 3.3511e-01, 9.6030e-02, 1.2662e-01],\n",
       "        [7.9392e-01, 8.8364e-01, 5.7753e-01, 2.7621e-01],\n",
       "        [2.7442e-01, 6.5832e-02, 6.7173e-01, 9.5915e-04],\n",
       "        [4.1093e-03, 5.2010e-02, 2.8162e-01, 2.2923e-02],\n",
       "        [1.9709e-01, 1.9770e-02, 7.5470e-01, 1.2059e-01],\n",
       "        [5.3050e-01, 5.2019e-02, 7.9658e-01, 4.1665e-02],\n",
       "        [8.6648e-02, 6.4176e-02, 2.2584e-01, 3.2761e-01],\n",
       "        [1.8724e-01, 6.1380e-02, 4.3004e-01, 1.3888e-01]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x1*x2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2180, 0.4560],\n",
       "         [0.2683, 0.7861],\n",
       "         [0.1366, 0.6557],\n",
       "         [0.5301, 0.0722],\n",
       "         [0.0418, 0.5974],\n",
       "         [0.2756, 0.6168],\n",
       "         [0.0366, 0.2499],\n",
       "         [0.6716, 0.2617],\n",
       "         [0.5410, 0.9365],\n",
       "         [0.4970, 0.0044],\n",
       "         [0.7760, 0.7816],\n",
       "         [0.6783, 0.9371],\n",
       "         [0.6999, 0.6441],\n",
       "         [0.0352, 0.3362],\n",
       "         [0.6288, 0.1161],\n",
       "         [0.3773, 0.8365],\n",
       "         [0.4917, 0.2034],\n",
       "         [0.2666, 0.4768],\n",
       "         [0.0364, 0.3881],\n",
       "         [0.8523, 0.4100],\n",
       "         [0.9767, 0.0418],\n",
       "         [0.6191, 0.9454],\n",
       "         [0.7303, 0.0668],\n",
       "         [0.2197, 0.7068],\n",
       "         [0.3767, 0.8824],\n",
       "         [0.8313, 0.4943],\n",
       "         [0.7406, 0.6138],\n",
       "         [0.3077, 0.0629]],\n",
       "\n",
       "        [[0.6501, 0.7336],\n",
       "         [0.5851, 0.4820],\n",
       "         [0.4774, 0.1265],\n",
       "         [0.4109, 0.1978],\n",
       "         [0.0731, 0.4156],\n",
       "         [0.7528, 0.3945],\n",
       "         [0.3843, 0.9922],\n",
       "         [0.6885, 0.5260],\n",
       "         [0.1152, 0.8106],\n",
       "         [0.9107, 0.7389],\n",
       "         [0.2196, 0.0843],\n",
       "         [0.1531, 0.2143],\n",
       "         [0.9518, 0.3972],\n",
       "         [0.3562, 0.6010],\n",
       "         [0.2320, 0.0385],\n",
       "         [0.4558, 0.0087],\n",
       "         [0.1363, 0.9451],\n",
       "         [0.8656, 0.9642],\n",
       "         [0.7782, 0.7875],\n",
       "         [0.3019, 0.1165],\n",
       "         [0.3503, 0.7836],\n",
       "         [0.9772, 0.7328],\n",
       "         [0.8040, 0.7108],\n",
       "         [0.9250, 0.1968],\n",
       "         [0.2882, 0.0304],\n",
       "         [0.0475, 0.7361],\n",
       "         [0.4414, 0.6557],\n",
       "         [0.1300, 0.1251]],\n",
       "\n",
       "        [[0.6744, 0.3781],\n",
       "         [0.7942, 0.0787],\n",
       "         [0.7583, 0.7179],\n",
       "         [0.2341, 0.3770],\n",
       "         [0.5025, 0.9053],\n",
       "         [0.4862, 0.2627],\n",
       "         [0.8502, 0.4446],\n",
       "         [0.6441, 0.4304],\n",
       "         [0.2492, 0.1403],\n",
       "         [0.3655, 0.9481],\n",
       "         [0.4209, 0.8969],\n",
       "         [0.9245, 0.7749],\n",
       "         [0.5395, 0.4638],\n",
       "         [0.4144, 0.4117],\n",
       "         [0.0870, 0.9237],\n",
       "         [0.8228, 0.3507],\n",
       "         [0.2834, 0.8848],\n",
       "         [0.7606, 0.1200],\n",
       "         [0.9383, 0.4505],\n",
       "         [0.5970, 0.3368],\n",
       "         [0.5127, 0.0948],\n",
       "         [0.9877, 0.7131],\n",
       "         [0.6439, 0.6665],\n",
       "         [0.4545, 0.0805],\n",
       "         [0.7016, 0.3141],\n",
       "         [0.2620, 0.2915],\n",
       "         [0.1265, 0.3889],\n",
       "         [0.7844, 0.2582]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tensor operation - slice a tensor; we can get 1 \"feature\" of our time series this way\n",
    "x1[:,:,0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example - vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get sample MNIST data using pytorch library\n",
    "\n",
    "batch_size_train=32\n",
    "batch_size_test=64\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/files/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize networks parameters\n",
    "MNIST_loss_func = F.nll_loss\n",
    "evaluate_every = 500\n",
    "non_decreasing_loss_to_stop = 3\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All this can be parametrized using a model config file, dictionary, or class\n",
    "\n",
    "class MNIST_conv_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        #This part of the class initializes the \"shape\" of our network, and creates the layers \n",
    "        #that have learned weights\n",
    "        super(MNIST_conv_net, self).__init__()\n",
    "        #input shape (Mnist images: batch size x 28 x 28 x 1)\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=5, stride=1)\n",
    "        #shape after conv1: 24 x 24 x 64 (lose 4 pixes because no padding)\n",
    "        #shape after max pool 1: 12 x 12 x 64 \n",
    "        #   (kernel size reduces the images dimension by 5-1,\n",
    "        #    max pool halves the image dimensions, channels is out_channels of our conv1 layer)\n",
    "        self.conv2 = nn.Conv2d(64, 32, kernel_size=3, stride=1)\n",
    "        #shape after conv2: 10 x 10 x 32\n",
    "        #after max pool 2: 5 x 5 x 32\n",
    "        self.drop1 = nn.Dropout()\n",
    "        self.fc1 = nn.Linear(800, 252)\n",
    "        self.drop2 = nn.Dropout()\n",
    "        self.fc2 = nn.Linear(252, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, 800)\n",
    "        x = self.drop1(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop2(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs, net):\n",
    "    early_stopping_hit = False\n",
    "    training_steps = []\n",
    "    training_losses = []\n",
    "    test_steps = []\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "    net.train()\n",
    "    consecutive_losses = 0\n",
    "    for e in range(n_epochs):\n",
    "        for i, (inputs, target) in enumerate(train_loader):\n",
    "            if torch.cuda.is_available:\n",
    "                inputs = inputs.to(torch.cuda.current_device())\n",
    "                target = target.to(torch.cuda.current_device())\n",
    "            optimizer.zero_grad()\n",
    "            output = net.forward(inputs)\n",
    "            loss = MNIST_loss_func(output, target)\n",
    "            loss.backward()\n",
    "            training_steps.append(e*len(train_loader) + i)\n",
    "            training_losses.append(loss.item())\n",
    "            optimizer.step()\n",
    "            if i % evaluate_every == 0:\n",
    "                print('Average training loss for last {} steps is: \\\n",
    "                      {}'.format(evaluate_every, sum(training_losses[-evaluate_every:]) / evaluate_every\n",
    "                ))\n",
    "                test_loss, pct_acc = test(net)\n",
    "                test_steps.append(e*len(train_loader) + i)\n",
    "                test_losses.append(test_loss)\n",
    "                test_accuracies.append(pct_acc)\n",
    "                if test_loss == min(test_losses):\n",
    "                    #Save the best model - optional, commented out\n",
    "                    #torch.save(net.state_dict(), '/mnist_model.pth')\n",
    "                    print('Improved on best loss')\n",
    "                    consecutive_losses = 0\n",
    "                else:\n",
    "                    print('Did not improve on best loss for {} steps'.format(consecutive_losses + 1))\n",
    "                    consecutive_losses += 1\n",
    "                if len(test_losses) >= 3 and min(test_losses[-non_decreasing_loss_to_stop:]) > min(test_losses):\n",
    "                    early_stopping_hit = True\n",
    "                    print ('Stopping training...')\n",
    "            if early_stopping_hit:\n",
    "                break\n",
    "        if early_stopping_hit:\n",
    "            break\n",
    "    return (training_steps, training_losses, test_steps, test_losses, test_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net):\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct_count = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, target in test_loader:\n",
    "            if torch.cuda.is_available:\n",
    "                inputs = inputs.to(torch.cuda.current_device())\n",
    "                target = target.to(torch.cuda.current_device())\n",
    "            output = net.forward(inputs)\n",
    "            test_loss += MNIST_loss_func(output, target, size_average=False).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct_count += pred.eq(target.data.view_as(pred)).sum()\n",
    "        test_loss = test_loss/len(test_loader.dataset)\n",
    "        print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct_count, len(test_loader.dataset),\n",
    "        100. * correct_count / len(test_loader.dataset)))\n",
    "        return(test_loss, correct_count / len(test_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: 0\n",
      "Average training loss for last 500 steps is:                       0.004630856037139893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliks\\anaconda3\\envs\\mmf_env\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 2.3124, Accuracy: 847/10000 (8%)\n",
      "\n",
      "Improved on best loss\n",
      "Average training loss for last 500 steps is:                       0.6122326492145658\n",
      "\n",
      "Test set: Avg. loss: 0.2105, Accuracy: 9365/10000 (94%)\n",
      "\n",
      "Improved on best loss\n",
      "Average training loss for last 500 steps is:                       0.15694427666440605\n",
      "\n",
      "Test set: Avg. loss: 0.1172, Accuracy: 9654/10000 (97%)\n",
      "\n",
      "Improved on best loss\n",
      "Average training loss for last 500 steps is:                       0.1261236136201769\n",
      "\n",
      "Test set: Avg. loss: 0.0969, Accuracy: 9700/10000 (97%)\n",
      "\n",
      "Improved on best loss\n",
      "Average training loss for last 500 steps is:                       0.09932576083205641\n",
      "\n",
      "Test set: Avg. loss: 0.0725, Accuracy: 9784/10000 (98%)\n",
      "\n",
      "Improved on best loss\n",
      "Average training loss for last 500 steps is:                       0.08043095098854974\n",
      "\n",
      "Test set: Avg. loss: 0.0742, Accuracy: 9769/10000 (98%)\n",
      "\n",
      "Did not improve on best loss for 1 steps\n",
      "Average training loss for last 500 steps is:                       0.07477310504810884\n",
      "\n",
      "Test set: Avg. loss: 0.0596, Accuracy: 9813/10000 (98%)\n",
      "\n",
      "Improved on best loss\n",
      "Average training loss for last 500 steps is:                       0.06831111305160448\n",
      "\n",
      "Test set: Avg. loss: 0.0571, Accuracy: 9818/10000 (98%)\n",
      "\n",
      "Improved on best loss\n",
      "Average training loss for last 500 steps is:                       0.06364275171840564\n",
      "\n",
      "Test set: Avg. loss: 0.0581, Accuracy: 9809/10000 (98%)\n",
      "\n",
      "Did not improve on best loss for 1 steps\n",
      "Average training loss for last 500 steps is:                       0.0549195803606417\n",
      "\n",
      "Test set: Avg. loss: 0.0552, Accuracy: 9825/10000 (98%)\n",
      "\n",
      "Improved on best loss\n",
      "Average training loss for last 500 steps is:                       0.05692156385281123\n",
      "\n",
      "Test set: Avg. loss: 0.0434, Accuracy: 9862/10000 (99%)\n",
      "\n",
      "Improved on best loss\n",
      "Average training loss for last 500 steps is:                       0.04888196427433286\n",
      "\n",
      "Test set: Avg. loss: 0.0428, Accuracy: 9863/10000 (99%)\n",
      "\n",
      "Improved on best loss\n",
      "Average training loss for last 500 steps is:                       0.04897416933136992\n",
      "\n",
      "Test set: Avg. loss: 0.0419, Accuracy: 9865/10000 (99%)\n",
      "\n",
      "Improved on best loss\n",
      "Average training loss for last 500 steps is:                       0.0415953658989165\n",
      "\n",
      "Test set: Avg. loss: 0.0355, Accuracy: 9884/10000 (99%)\n",
      "\n",
      "Improved on best loss\n",
      "Average training loss for last 500 steps is:                       0.038837499790533914\n",
      "\n",
      "Test set: Avg. loss: 0.0411, Accuracy: 9868/10000 (99%)\n",
      "\n",
      "Did not improve on best loss for 1 steps\n",
      "Average training loss for last 500 steps is:                       0.04179134359350428\n",
      "\n",
      "Test set: Avg. loss: 0.0364, Accuracy: 9876/10000 (99%)\n",
      "\n",
      "Did not improve on best loss for 2 steps\n",
      "Average training loss for last 500 steps is:                       0.03973529691051226\n",
      "\n",
      "Test set: Avg. loss: 0.0321, Accuracy: 9890/10000 (99%)\n",
      "\n",
      "Improved on best loss\n",
      "Average training loss for last 500 steps is:                       0.033040701023535804\n",
      "\n",
      "Test set: Avg. loss: 0.0318, Accuracy: 9896/10000 (99%)\n",
      "\n",
      "Improved on best loss\n",
      "Average training loss for last 500 steps is:                       0.033278999718779234\n",
      "\n",
      "Test set: Avg. loss: 0.0283, Accuracy: 9895/10000 (99%)\n",
      "\n",
      "Improved on best loss\n",
      "Average training loss for last 500 steps is:                       0.03145487438398414\n",
      "\n",
      "Test set: Avg. loss: 0.0289, Accuracy: 9895/10000 (99%)\n",
      "\n",
      "Did not improve on best loss for 1 steps\n",
      "Average training loss for last 500 steps is:                       0.03257269473440829\n",
      "\n",
      "Test set: Avg. loss: 0.0279, Accuracy: 9905/10000 (99%)\n",
      "\n",
      "Improved on best loss\n",
      "Average training loss for last 500 steps is:                       0.024869927663297858\n",
      "\n",
      "Test set: Avg. loss: 0.0316, Accuracy: 9899/10000 (99%)\n",
      "\n",
      "Did not improve on best loss for 1 steps\n",
      "Average training loss for last 500 steps is:                       0.027218428270789446\n",
      "\n",
      "Test set: Avg. loss: 0.0317, Accuracy: 9896/10000 (99%)\n",
      "\n",
      "Did not improve on best loss for 2 steps\n",
      "Average training loss for last 500 steps is:                       0.028179275393282296\n",
      "\n",
      "Test set: Avg. loss: 0.0306, Accuracy: 9897/10000 (99%)\n",
      "\n",
      "Did not improve on best loss for 3 steps\n",
      "Stopping training...\n"
     ]
    }
   ],
   "source": [
    "net = MNIST_conv_net()\n",
    "if torch.cuda.is_available:\n",
    "    device = torch.cuda.current_device()\n",
    "    net.to(device)\n",
    "    print('Using GPU: {}'.format(device))\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "training_steps, training_losses, test_steps, test_losses, test_accuracies = train(10, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 12))\n",
    "#plt.plot(training_steps, training_losses, color='teal')\n",
    "plt.scatter(test_steps, test_losses, color='purple')\n",
    "plt.legend(['Train loss (batch)', 'Test loss (average)'], loc='upper right')\n",
    "plt.xlabel('Train steps')\n",
    "plt.ylabel('Loss (negative log likelihood)')\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take a look at the images as a sanity check\n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy() \n",
    "fig = plt.figure(figsize=(20, 4))\n",
    "for idx in np.arange(10):\n",
    "    ax = fig.add_subplot(2, 10/2, idx+1, xticks=[], yticks=[])\n",
    "    plt.imshow(np.transpose(images[idx], (1, 2, 0)), cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmf_env",
   "language": "python",
   "name": "mmf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
